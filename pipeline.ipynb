{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8bc14a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\SHASHI\n",
      "[nltk_data]     GOWDA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['science', 'amazing', 'planet', 'animals', 'filllm', 'internet', 'smallest', 'whenever', 'accessibility', 'initially']\n",
      "['https://www.livescience.com/3945-history-dinosaurs.html\\n', 'https://www.livescience.com/archaeology-to-watch-in-2022\\n', 'https://www.discovery.com/nature\\n', 'https://www.nature.com/articles/485039a']\n",
      "URL ====>  https://www.livescience.com/3945-history-dinosaurs.html\n",
      "\n",
      "URL ====>  https://www.livescience.com/archaeology-to-watch-in-2022\n",
      "\n",
      "URL ====>  https://www.discovery.com/nature\n",
      "\n",
      "URL ====>  https://www.nature.com/articles/485039a\n",
      "**********************************************************************************\n",
      "Top three words are:\n",
      "science- 7\n",
      "planet- 6\n",
      "amazing- 5\n",
      "**********************************************************************************\n",
      "Total number of occurance of each word in the list \n",
      "science- 7\n",
      "planet- 6\n",
      "amazing- 5\n",
      "animals- 5\n",
      "accessibility- 4\n",
      "smallest- 2\n",
      "internet- 2\n",
      "whenever- 1\n",
      "initially- 1\n",
      "filllm- 0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "urls_text_file_path = r\"E:\\noknok\\urls.txt\"\n",
    "test_words_file_path = r\"E:\\noknok\\words.txt\"\n",
    "\n",
    "f_ptr_url = open(urls_text_file_path,\"r\")\n",
    "w_ptr_url = open(test_words_file_path,\"r\")\n",
    "urls_data = f_ptr_url.readlines()\n",
    "words = w_ptr_url.readlines()\n",
    "words_data=[re.sub(r'\\n', '', w) for w in words ]\n",
    "print(words_data)\n",
    "print(urls_data)\n",
    "\n",
    "\n",
    "def remove_stop_words(sentence):\n",
    "    # print(\"sentence ===\",sentence)\n",
    "    word_tokens = word_tokenize(sentence)\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    return filtered_sentence\n",
    "\n",
    "def parse_output(c_p_obj):\n",
    "    word_list = []\n",
    "    for x in c_p_obj:\n",
    "        key = x\n",
    "        value = c_p_obj[key]\n",
    "        if len(key) > 5 and (bool(re.match('^[a-zA-Z0-9]*$', key)) == True):\n",
    "            word_list.append(key)\n",
    "\n",
    "    sentence = ' '.join(word for word in word_list)\n",
    "    filtered_sentence = remove_stop_words(sentence)\n",
    "    return Counter(filtered_sentence)\n",
    "\n",
    "def read_and_process_url(url):\n",
    "\n",
    "    print(\"URL ====> \",url)\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content, features=\"html.parser\")\n",
    "\n",
    "    text_p = (''.join(s.findAll(text=True))for s in soup.findAll('p'))\n",
    "\n",
    "\n",
    "    c_p_obj = Counter((x.rstrip(punctuation).lower() for y in text_p for x in y.split()))\n",
    "\n",
    "    c_p = parse_output(c_p_obj)\n",
    "\n",
    "\n",
    "    # We get the words within divs\n",
    "    text_div = (''.join(s.findAll(text=True))for s in soup.findAll('div'))\n",
    "    \n",
    "    c_div_obj = Counter((x.rstrip(punctuation).lower() for y in text_div for x in y.split()))\n",
    "    c_div = parse_output(c_div_obj)\n",
    "    # We sum the two counters and get a list with words count from most to less common\n",
    "    total = c_div + c_p\n",
    "    list_most_common_words = total.most_common() \n",
    "\n",
    "    #print(list_most_common_words)\n",
    "    return list_most_common_words\n",
    "\n",
    "def process_output(output_list):\n",
    "    result_dict = {}\n",
    "#     for word in words_data:\n",
    "#         if word in output_list:\n",
    "#             if word not in result_dict.keys():\n",
    "#                 result_dict[word] = 1\n",
    "#             else:\n",
    "#                 result_dict[word] += 1\n",
    "#     return result_dict\n",
    "    \n",
    "\n",
    "    for common_words in output_list:\n",
    "        for x in common_words:\n",
    "            word = x[0]\n",
    "            value = x[1]\n",
    "\n",
    "            if word in words_data:\n",
    "                if word not in result_dict.keys():\n",
    "                    result_dict[word] = value\n",
    "                else:\n",
    "                    result_dict[word] +=  value\n",
    "    for word in words_data:\n",
    "        if word not in result_dict:\n",
    "            result_dict[word]=0\n",
    "  \n",
    "\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "def main():\n",
    "    output_list = []\n",
    "    for url in urls_data:\n",
    "        common_words = read_and_process_url(url)\n",
    "        output_list.append(common_words)\n",
    "#         print(output_list)\n",
    "\n",
    "    results = process_output(output_list)\n",
    "    print(\"**********************************************************************************\")\n",
    "    print(\"Top three words are:\")\n",
    "    results=Counter(results)\n",
    "    results.most_common()\n",
    "    for k, v in results.most_common(3):\n",
    "        print('%s- %i' % (k, v))\n",
    "        \n",
    "    print(\"**********************************************************************************\")\n",
    "    print(\"Total number of occurance of each word in the list \")\n",
    "    for k, v in results.most_common():\n",
    "        print('%s- %i' % (k, v))\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d02bb12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
